p
# QUESTION 1
N <- 20
mean <- 9.5
variance <- 6.76
error <- qnorm(0.975)*sqrt(variance/N)
left <- mean-error
right <- mean+error
left
right
N <- 20
sample_mean <- 9.5
known_variance <- 6.76
mu0 <- 10    #hypothesized population mean
#calculate the observed value of Z to be able to calculate p-value
Zobs <- (samplemean-mu0)/sqrt(known_variance/N)
N <- 20
sample_mean <- 9.5
known_variance <- 6.76
mu0 <- 10    #hypothesized population mean
#calculate the observed value of Z to be able to calculate p-value
Zobs <- (sample_mean-mu0)/sqrt(known_variance/N)
p <- pnorm(Zobs)   #w don't add lower.tail=FALSE as we need the area to the left of the number (Zobs= -0.86)
p
N <- 20
sample_mean <- 9.5
known_variance <- 6.76
mu0 <- 10    #hypothesized population mean
#calculate the observed value of Z to be able to calculate p-value
Zobs <- (sample_mean-mu0)/sqrt(known_variance/N)
Zobs
mu_0 <- 8.9
Z_obs <- (mean-mu_0)/sqrt(known_variance/N)
p <- pnorm(-Z_obs)+pnorm(Z_obs, lower.tail=FALSE)
p
error <- qnorm(1-0.01/2)*sqrt(variance/N)
left <- mean-error
right <- mean+error
left
right
pop_variance <- 0.2401
N <- 37
mean <- 7.42
error <- qnorm(1-0.01/2)*sqrt(pop_variance/N)
left <- mean-error
right <- mean+error
left
right
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
right <- mean+error
left
right
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
population_mean <- 7.7
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
# QUESTION 1
N <- 20
mean <- 9.5
variance <- 6.76
error <- qnorm(0.975)*sqrt(variance/N)
left <- mean-error
right <- mean+error
left   #8.360521
right  #10.63948
# QUESTION 1 b
#One-sided alternative hypothesis as we check is mu is less than 10.
N <- 20
sample_mean <- 9.5
known_variance <- 6.76
mu0 <- 10    #hypothesized population mean
#calculate the observed value of Z to be able to calculate p-value
Zobs <- (sample_mean-mu0)/sqrt(known_variance/N)
p <- pnorm(Zobs)   #we don't add lower.tail=FALSE as we need the area to the left of the number (Zobs= -0.86)
p
# QUESTION 1 c
#Two-sided alternative hypothesis
mu_0 <- 8.9
Z_obs <- (mean-mu_0)/sqrt(known_variance/N)
p <- pnorm(-Z_obs)+pnorm(Z_obs, lower.tail=FALSE)
p      #p=0.3020574 Thus, not enough evidence to reject null hypothesis at 5% level
error <- qnorm(1-0.01/2)*sqrt(variance/N)
left <- mean-error
right <- mean+error
left    #8.00247
right   #10.99753
#Critical area is smaller than 8 and larger than 11.
#QUESTION 2a
#population variance unknown
sample_variance <- 0.2401
N <- 37
standard_error <- sqrt(sample_variance/N)
standard_error   #0.0805555
#QUESTION 2b
# It is a T distribution on n-1 degrees of freedom
#QUESTION 2c i
#population variance is known
pop_variance <- 0.2401
N <- 37
mean <- 7.42
error <- qnorm(1-0.01/2)*sqrt(pop_variance/N)
left <- mean-error
right <- mean+error
left   #7.212503
right  #7.627497
#The 99% confidence interval is [7.21, 7.63]
#QUESTION 2c ii
#population variance unknown
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
right <- mean+error
left   #7.200931
right  #7.639069
#The 99% confidence interval is [7.20, 7.64]
#QUESTION 2d
population_mean <- 7.7
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- population_mean-error
left #So we reject values that are higher than 7.48????????
#QUESTION 3
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/N  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
MLE   #2.64
#QUESTION 4a&b
#probability of exactly 4 faults
#(X=4|X~Pois(3))
p <- dpois(4, lambda=3)
p   #0.1680314
#QUESTION 2a
#population variance unknown
sample_variance <- 0.2401
N <- 37
standard_error <- sqrt(sample_variance/N)
standard_error   #0.0805555
#QUESTION 2b
# It is a T distribution on n-1 degrees of freedom
#QUESTION 2c i
#population variance is known
pop_variance <- 0.2401
N <- 37
mean <- 7.42
error <- qnorm(1-0.01/2)*sqrt(pop_variance/N)
left <- mean-error
right <- mean+error
left   #7.212503
right  #7.627497
#The 99% confidence interval is [7.21, 7.63]
#QUESTION 2c ii
#population variance unknown
sample_variance <- 0.2401
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- mean-error
right <- mean+error
left   #7.200931
right  #7.639069
#The 99% confidence interval is [7.20, 7.64]
#QUESTION 2d
population_mean <- 7.7
error <- qt(1-0.01/2, df=N-1)*sqrt(sample_variance/N)
left <- population_mean-error
left
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/6N  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/6*N  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
MLE
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/N/6  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
MLE
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/(N/6)  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
MLE
#QUESTION 3
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
MLE <- sum(x*Obs)/N/6  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
MLE   #2.64
#QUESTION 3
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
mean <- sum(x*Obs)/6  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
mean
x <- c(0,1,2,3,4,5,6)
Obs <- c(14,23,50,64,36,9,4)   #the observed frequencies
N <- sum(Obs)  #total number of tosses
N
Expected <- rep(N/7, 7)   #the expected number of heads
Expected
X2 <- sum((Obs-Expected)^2/Expected)
p <- pchisq(X2, 6, lower.tail = FALSE)
p    #2.277505e-20
#Extremely strong evidence that H0 couldn't come from Binomial distribution, but HA could.
mean <- sum(x*Obs)/N  #(23 + 2x50 + 3x64 + 4x36 + 5x9 + 6x4) / 200
mean
install.packages("faraway")
library(faraway)
set.seed(234) # To make it reproducible
group_labels <- c("A", "B", "C") # a factor with three levels
meanA <- 5
meanB <- 7
meanC <- 7
sd_common <- 1
N <- 4 # 4 observations per group
A <- round(rnorm(N, meanA, sd_common), 1)
B <- round(rnorm(N, meanB, sd_common), 1)
C <- round(rnorm(N, meanC, sd_common), 1)
group <- rep(group_labels, rep(N, 3))
Y <- c(A, B, C)
df <- data.frame(group = group, Y = Y)
head(df)
lm1 <- lm(Y ~ group, data=df)
anova(lm1)
meteorology<-read.csv("data/meteorology.csv",row.names=1)
setwd("C:/Users/Pati/OneDrive - Aberystwyth University/STATS/WORKBOOKS/PCA")
meteorology<-read.csv("data/meteorology.csv",row.names=1)
meteorology<-read.csv("meteorology.csv",row.names=1)
met.pca<-prcomp(meteorology)
View(met.pca)
View(met.pca)
View(meteorology)
View(meteorology)
met.pca$center
met.pca$x
met.pca$rotation
met.pca$sdev
#b
proportions <- met.pca$sdev^2/sum(met.pca$sdev^2)
proportions
library(ggplot2)
qplot(c(1:5), proportions) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
met.pca.scores.df <- data.frame(met.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
#d
met.pca<-prcomp(meteorology, scale=TRUE)
met.pca$center
met.pca$x
met.pca$rotation
met.pca$sdev
met.pca<-prcomp(meteorology, scale=TRUE)
met.pca$center
met.pca$x
met.pca$rotation
met.pca$sdev
#b
proportions <- met.pca$sdev^2/sum(met.pca$sdev^2)
proportions
library(ggplot2)
#SCREE PLOT
qplot(c(1:5), proportions) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
met.pca.scores.df <- data.frame(met.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
library(bootstrap)
install.packages(bootstrap)
installed.packages(bootstrap)
installed.packages("bootstrap")
library(bootstrap)
head(scor)   # the data frame is called scor and the head() function shows you the top six rows – useful for getting a sense of the shape of a data set
installed.packages('bootstrap')
library(bootstrap)
head(scor)   # the data frame is called scor and the head() function shows you the top six rows – useful for getting a sense of the shape of a data set
met.pca<-prcomp(meteorology, scale=TRUE)
proportions <- met.pca$sdev^2/sum(met.pca$sdev^2)
proportions
met.pca.scores.df <- data.frame(met.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
met.pca.scores.df <- data.frame(met.pca$x)
p2 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC3")
p2
met.pca.scores.df <- data.frame(met.pca$x)
p3 <- ggplot(met.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p3
met.pca.scores.df <- data.frame(met.pca$x)
p2 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
met.pca.scores.df <- data.frame(met.pca$x)
p2 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
met.pca.scores.df <- data.frame(met.pca$x)
p3 <- ggplot(met.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
met.pca.scores.df <- data.frame(met.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
head(scor)   # the data frame is called scor and the head() function shows you the top six rows – useful for getting a sense of the shape of a data set
met.pca<-prcomp(meteorology, scale=TRUE)
proportions <- met.pca$sdev^2/sum(met.pca$sdev^2)
proportions
met.pca.scores.df <- data.frame(met.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
head(scor)   # the data frame is called scor and the head() function shows you the top six rows – useful for getting a sense of the shape of a data set
scor.pca<-prcomp(scor, scale=TRUE)
proportions <- scor.pca$sdev^2/sum(scor.pca$sdev^2)
proportions
scor.pca.scores.df <- data.frame(scor.pca$x)
p1 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
scor.pca.scores.df <- data.frame(scor.pca$x)
p2 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
scor.pca.scores.df <- data.frame(scor.pca$x)
p3 <- ggplot(scor.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
scor.pca.scores.df <- data.frame(scor.pca$x)
p3 <- ggplot(scor.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC2 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
scor.pca.scores.df <- data.frame(scor.pca$x)
p2 <- ggplot(met.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
scor.pca.scores.df <- data.frame(scor.pca$x)
p1 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
scor.pca.scores.df <- data.frame(scor.pca$x)
p1 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
scor.pca.scores.df <- data.frame(scor.pca$x)
p2 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
scor.pca.scores.df <- data.frame(scor.pca$x)
p3 <- ggplot(scor.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC2 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
qplot(c(1:5), proportions) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
scor.pca.scores.df <- data.frame(scor.pca$x)
p3 <- ggplot(scor.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[3], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
scor.pca.scores.df <- data.frame(scor.pca$x)
p1 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
scor.pca.scores.df <- data.frame(scor.pca$x)
p1 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC2)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)")) +
ggtitle("PC2 against PC1")
p1
scor.pca.scores.df <- data.frame(scor.pca$x)
p2 <- ggplot(scor.pca.scores.df, aes(x=PC1, y=PC3)) +
geom_point() +
labs(x=paste0("PC1 (", 100*round(proportions[1], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[3], 2), "% of variability)")) +
ggtitle("PC3 against PC1")
p2
scor.pca.scores.df <- data.frame(scor.pca$x)
p3 <- ggplot(scor.pca.scores.df, aes(x=PC2, y=PC3)) +
geom_point() +
labs(x=paste0("PC2 (", 100*round(proportions[2], 2), "% of variability)"), y=paste0("PC3 (", 100*round(proportions[3], 2), "% of variability)")) +
ggtitle("PC3 against PC2")
p3
rotation <- scor.pca$rotation
rotation
qplot(c(1:5), proportions) +
geom_line() +
xlab("Principal Component") +
ylab("Variance Explained") +
ggtitle("Scree Plot") +
ylim(0, 1)
